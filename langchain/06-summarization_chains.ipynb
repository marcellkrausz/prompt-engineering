{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4 langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the openai secret key:\n",
    "import getpass\n",
    "\n",
    "secret_key = getpass.getpass('Please enter your openai key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "chat_model = init_chat_model(model_provider=\"openai\", model=\"gpt-4o-mini\", api_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article \"LLM Powered Autonomous Agents\" by Lilian Weng explores the concept and development of autonomous agents leveraging large language models (LLMs) as their core controller. It outlines an agent system framework consisting of three main components: \n",
      "\n",
      "1. **Planning**: This involves task decomposition, allowing agents to break down complex tasks into manageable subgoals, and self-reflection mechanisms to improve future actions based on past performance.\n",
      "\n",
      "2. **Memory**: It categorizes memory into short-term and long-term types, highlighting the importance of memory in enabling agents to retain and retrieve information efficiently to enhance their decision-making capabilities.\n",
      "\n",
      "3. **Tool Use**: It discusses various case studies where LLMs are integrated with external tools for improved functionality, such as in scientific discovery agents, and generative agents that simulate human behavior.\n",
      "\n",
      "The article also examines the challenges faced in building these LLM-powered agents, including the constraints of finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. In conclusion, Weng cites various studies and demonstrations like AutoGPT and GPT-Engineer, showcasing the potential and ongoing advancements in this field.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
    ")\n",
    "\n",
    "# Instantiate chain\n",
    "chain = create_stuff_documents_chain(chat_model, prompt)\n",
    "\n",
    "# Invoke chain\n",
    "result = chain.invoke({\"context\": docs})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| article| \"|LL|M| Powered| Autonomous| Agents|\"| by| Lil|ian| W|eng| discusses| the| development| and| potential| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| provides| an| overview| of| the| system| architecture| that| integrates| planning|,| memory|,| and| tool| use| as| core| components|.\n",
      "\n",
      "|1|.| **|Agent| System| Overview|**|:| L|LM|s| serve| as| the| brain| of| autonomous| agents|.| Key| components| include|:\n",
      "|  | -| **|Planning|**|:| In|vol|ves| task| decomposition| and| self|-ref|lection| to| manage| complex| tasks| effectively| and| learn| from| past| actions|.\n",
      "|  | -| **|Memory|**|:| Differ|enti|ates| between| short|-term| memory| (|in|-context| learning|)| and| long|-term| memory| (|external| information| storage|)| to| sustain| infinite| recall| capabilities| through| vector| stores| and| fast| retrieval| algorithms|.\n",
      "|  | -| **|Tool| Use|**|:| Enh|ances| agent| capabilities| by| allowing| them| to| utilize| external| APIs| and| tools| to| gather| information| or| execute| commands|.\n",
      "\n",
      "|2|.| **|Challenges|**|:| The| article| identifies| common| limitations| such| as|:\n",
      "|  | -| Fin|ite| context| lengths| restricting| the| agent|'s| ability| to| store| and| retrieve| comprehensive| historical| information|.\n",
      "|  | -| Difficult|ies| in| long|-term| planning| and| adapting| to| unforeseen| errors|.\n",
      "|  | -| Reliability| concerns| regarding| natural| language| interfaces| that| can| lead| to| formatting| errors| and| inconsist|encies|.\n",
      "\n",
      "|3|.| **|Case| Studies| and| Examples|**|:| The| article| presents| various| proof|-of|-con|cept| models| like| Auto|GPT| and| GPT|-|Engineer|,| showcasing| their| functionalities| in| task| management| and| software| development|.\n",
      "\n",
      "|In| summary|,| L|LM|-powered| autonomous| agents| represent| a| significant| advancement| in| AI|,| potentially| transforming| how| complex| problem|-solving| tasks| are| approached|,| although| challenges| remain| in| achieving| optimal| performance| and| reliability|.||"
     ]
    }
   ],
   "source": [
    "#Stream tokens\n",
    "for token in chain.stream({\"context\": docs}):\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['docs'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['docs'], input_types={}, partial_variables={}, template='\\nThe following is a set of summaries:\\n{docs}\\nTake these and distill it into a final, consolidated summary\\nof the main themes.\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Also available via the hub: `hub.pull(\"rlm/reduce-prompt\")`\n",
    "reduce_template = \"\"\"\n",
    "The following is a set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary\n",
    "of the main themes.\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
    "print(reduce_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1003, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 14 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Generated {len(split_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Write a concise summary of the following:\\\\n\\\\n{context}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
    ")\n",
    "\n",
    "print(map_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Literal, TypedDict\n",
    "\n",
    "from langchain.chains.combine_documents.reduce import (\n",
    "    acollapse_docs,\n",
    "    split_list_of_docs,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "token_max = 1000\n",
    "\n",
    "\n",
    "def length_function(documents: List[Document]) -> int:\n",
    "    \"\"\"Get number of tokens for input contents.\"\"\"\n",
    "    return sum(chat_model.get_num_tokens(doc.page_content) for doc in documents)\n",
    "\n",
    "\n",
    "# This will be the overall state of the main graph.\n",
    "# It will contain the input document contents, corresponding\n",
    "# summaries, and a final summary.\n",
    "class OverallState(TypedDict):\n",
    "    # Notice here we use the operator.add\n",
    "    # This is because we want combine all the summaries we generate\n",
    "    # from individual nodes back into one list - this is essentially\n",
    "    # the \"reduce\" part\n",
    "    contents: List[str]\n",
    "    summaries: Annotated[list, operator.add]\n",
    "    collapsed_summaries: List[Document]\n",
    "    final_summary: str\n",
    "\n",
    "\n",
    "# This will be the state of the node that we will \"map\" all\n",
    "# documents to in order to generate summaries\n",
    "class SummaryState(TypedDict):\n",
    "    content: str\n",
    "\n",
    "\n",
    "# Here we generate a summary, given a document\n",
    "async def generate_summary(state: SummaryState):\n",
    "    #generate a promt to the llm model\n",
    "    prompt = map_prompt.invoke(state[\"content\"])\n",
    "    response = await chat_model.ainvoke(prompt)\n",
    "    # We return a dictionary with a key \"summaries\" and a list of summaries\n",
    "    return {\"summaries\": [response.content]}\n",
    "\n",
    "\n",
    "# Here we define the logic to map out over the documents\n",
    "# We will use this an edge in the graph\n",
    "def map_summaries(state: OverallState):\n",
    "    # We will return a list of `Send` objects\n",
    "    # Each `Send` object consists of the name of a node in the graph\n",
    "    # as well as the state to send to that node\n",
    "    return [\n",
    "        # send is a special function that will send the data to the next node\n",
    "        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "def collect_summaries(state: OverallState):\n",
    "    return {\n",
    "        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]\n",
    "    }\n",
    "\n",
    "\n",
    "async def _reduce(input: dict) -> str:\n",
    "    prompt = reduce_prompt.invoke(input)\n",
    "    response = await chat_model.ainvoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Add node to collapse summaries\n",
    "async def collapse_summaries(state: OverallState):\n",
    "    doc_lists = split_list_of_docs(\n",
    "        state[\"collapsed_summaries\"], length_function, token_max\n",
    "    )\n",
    "    results = []\n",
    "    for doc_list in doc_lists:\n",
    "        results.append(await acollapse_docs(doc_list, _reduce))\n",
    "\n",
    "    return {\"collapsed_summaries\": results}\n",
    "\n",
    "\n",
    "# This represents a conditional edge in the graph that determines\n",
    "# if we should collapse the summaries or not\n",
    "def should_collapse(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
    "    num_tokens = length_function(state[\"collapsed_summaries\"])\n",
    "    if num_tokens > token_max:\n",
    "        return \"collapse_summaries\"\n",
    "    else:\n",
    "        return \"generate_final_summary\"\n",
    "\n",
    "\n",
    "# Here we will generate the final summary\n",
    "async def generate_final_summary(state: OverallState):\n",
    "    response = await _reduce(state[\"collapsed_summaries\"])\n",
    "    return {\"final_summary\": response}\n",
    "\n",
    "\n",
    "# Construct the graph\n",
    "# Nodes:\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_summary\", generate_summary)  # same as before\n",
    "graph.add_node(\"collect_summaries\", collect_summaries)\n",
    "graph.add_node(\"collapse_summaries\", collapse_summaries)\n",
    "graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "\n",
    "# Edges:\n",
    "graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "graph.add_edge(\"generate_summary\", \"collect_summaries\")\n",
    "graph.add_conditional_edges(\"collect_summaries\", should_collapse)\n",
    "graph.add_conditional_edges(\"collapse_summaries\", should_collapse)\n",
    "graph.add_edge(\"generate_final_summary\", END)\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['collect_summaries']\n",
      "['collapse_summaries']\n",
      "['collapse_summaries']\n",
      "['generate_final_summary']\n"
     ]
    }
   ],
   "source": [
    "async for step in app.astream(\n",
    "    {\"contents\": [doc.page_content for doc in split_docs]},\n",
    "    {\"recursion_limit\": 10},\n",
    "):\n",
    "    print(list(step.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_final_summary': {'final_summary': 'The consolidated summary of the main themes from the two documents highlights key aspects of LLM-powered autonomous agents and software architecture implementation:\\n\\n1. **Core Components of LLM-Powered Agents**: These agents are built around critical elements such as Planning, Memory, and Tool Use. Effective planning techniques like Chain of Thought and Tree of Thoughts are essential for breaking down complex tasks, alongside mechanisms for self-reflection and iterative learning to improve performance.\\n\\n2. **Addressing Challenges in LLMs**: The importance of managing hallucinations is emphasized, with techniques like Chain of Hindsight and Algorithm Distillation enhancing output quality and supporting policy development through reinforcement learning. Additionally, limitations in LLMs related to self-reflection and long-term planning are noted, particularly in adapting to historical information and errors.\\n\\n3. **Memory and Learning Processes**: The discussion includes various memory types related to human cognition—such as sensory, short-term, and long-term—and algorithms that optimize memory searches (e.g., LSH and HNSW) to enhance LLM capabilities.\\n\\n4. **Software Architecture Implementation**: The summary addresses a structured approach to translating software architecture into executable code, focusing on core components like EntryPoint, Models, and Controllers, along with best practices in coding standards, clarity, documentation, and dependency management.\\n\\n5. **Advancements in Autonomous Agents**: There is an ongoing exploration of LLMs in developing autonomous agents, discussing techniques that enhance reasoning and problem-solving capabilities, indicative of their growing role in automation.\\n\\n6. **Practical Applications and Game Development**: Applications span various domains, including drug discovery and generative agents simulating human interaction. Additionally, frameworks in game development stress the need for clarity in user requirements and game mechanics to enhance experience and effectiveness.\\n\\nOverall, the themes underscore the versatility, potential, and challenges faced by LLMs and autonomous agents in enhancing decision-making, software development, and automation, alongside the importance of structured methodologies in software architecture.'}}\n"
     ]
    }
   ],
   "source": [
    "print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'final_summary': 'The consolidated summary of the main themes from the two documents highlights key aspects of LLM-powered autonomous agents and software architecture implementation:\\n\\n1. **Core Components of LLM-Powered Agents**: These agents are built around critical elements such as Planning, Memory, and Tool Use. Effective planning techniques like Chain of Thought and Tree of Thoughts are essential for breaking down complex tasks, alongside mechanisms for self-reflection and iterative learning to improve performance.\\n\\n2. **Addressing Challenges in LLMs**: The importance of managing hallucinations is emphasized, with techniques like Chain of Hindsight and Algorithm Distillation enhancing output quality and supporting policy development through reinforcement learning. Additionally, limitations in LLMs related to self-reflection and long-term planning are noted, particularly in adapting to historical information and errors.\\n\\n3. **Memory and Learning Processes**: The discussion includes various memory types related to human cognition—such as sensory, short-term, and long-term—and algorithms that optimize memory searches (e.g., LSH and HNSW) to enhance LLM capabilities.\\n\\n4. **Software Architecture Implementation**: The summary addresses a structured approach to translating software architecture into executable code, focusing on core components like EntryPoint, Models, and Controllers, along with best practices in coding standards, clarity, documentation, and dependency management.\\n\\n5. **Advancements in Autonomous Agents**: There is an ongoing exploration of LLMs in developing autonomous agents, discussing techniques that enhance reasoning and problem-solving capabilities, indicative of their growing role in automation.\\n\\n6. **Practical Applications and Game Development**: Applications span various domains, including drug discovery and generative agents simulating human interaction. Additionally, frameworks in game development stress the need for clarity in user requirements and game mechanics to enhance experience and effectiveness.\\n\\nOverall, the themes underscore the versatility, potential, and challenges faced by LLMs and autonomous agents in enhancing decision-making, software development, and automation, alongside the importance of structured methodologies in software architecture.'}\n"
     ]
    }
   ],
   "source": [
    "print(step[\"generate_final_summary\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

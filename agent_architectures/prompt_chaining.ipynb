{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b5e3b6",
   "metadata": {},
   "source": [
    "## **Workflow: Prompt chaining**\n",
    "\n",
    "Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one.\n",
    "\n",
    "You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.\n",
    "\n",
    "![Prompt chaining](resources/prompt-chaining.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e9814",
   "metadata": {},
   "source": [
    "## **Use cases:**\n",
    "\n",
    "- Generating Marketing copy, then translating it into a different language.\n",
    "- Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.\n",
    "- Using an LLM to clean and standardize raw data, then passing the cleaned data to another LLM for insights, summaries, or visualizations.\n",
    "- Generating a set of detailed questions based on a topic with one LLM, then passing those questions to another LLM to produce well-researched answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai pydantic --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfa2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b526b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7fb422",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82661ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Story(BaseModel):\n",
    "    story: str\n",
    "    title: str\n",
    "    author: str\n",
    "\n",
    "class StoryPlot(BaseModel):\n",
    "    plot: str\n",
    "\n",
    "class StoryPlots(BaseModel):\n",
    "    plots: List[StoryPlot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c74edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_chain_workflow(topic: str, num_stories: int) -> List[Story]:\n",
    "    \"\"\"Run a serial chain of LLM calls to address the `input_query\"\"\"\n",
    "\n",
    "    stories_plot_prompt = f'''Generate {num_stories} plot ideas for stories about {topic}.\n",
    "    These should be short and concise, and should be suitable for a children's book.\n",
    "    Later on these plot ideas will be used to generate full stories.'''\n",
    "\n",
    "    stories_plot_response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": stories_plot_prompt}],\n",
    "        temperature=0.5,\n",
    "        max_tokens=1000,\n",
    "        response_format=StoryPlots\n",
    "    )\n",
    "\n",
    "    if (stories_plot_response.choices[0].message.parsed.plots == []):\n",
    "        raise ValueError(\"No plots generated\")\n",
    "\n",
    "    print(\"Plots:\")\n",
    "    print(stories_plot_response.choices[0].message.parsed.plots)\n",
    "\n",
    "    # For each story plot, generate a full story\n",
    "    stories = []\n",
    "    for plot in stories_plot_response.choices[0].message.parsed.plots:\n",
    "        story_prompt = f'''Generate a full story based on the following plot: {plot.plot}.\n",
    "        The story should be suitable for a children's book. The title should not be within the story part of the response.\n",
    "        If there is no author, then write \"Unknown\"'''\n",
    "\n",
    "        story_response = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": story_prompt}],\n",
    "            temperature=0.5,\n",
    "            max_tokens=1000,\n",
    "            response_format=Story\n",
    "        )\n",
    "\n",
    "        if (story_response.choices[0].message.parsed.story == \"\"):\n",
    "            raise ValueError(\"No story generated\")\n",
    "\n",
    "        stories.append(story_response.choices[0].message.parsed)\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5528ba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots:\n",
      "[StoryPlot(plot='Levente, the wise senior developer, discovers that the code for a magical app has gone missing! With the help of his friends, a curious young coder named Mia and a playful robot named Byte, they embark on a thrilling adventure through the digital world to find the lost code. Along the way, they learn about teamwork, creativity, and the importance of sharing knowledge.'), StoryPlot(plot=\"When a mysterious bug starts causing chaos in the coding world, Levente the senior developer must rally his team of young programmers to solve the problem. Together, they build a 'Bug-Busting' machine that helps them understand the importance of debugging and perseverance. Through fun challenges and clever problem-solving, they turn the bug into a feature that helps everyone!\")]\n"
     ]
    }
   ],
   "source": [
    "stories = serial_chain_workflow(\"The senior dev, Levente\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53099297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cloudy Conundrum\n",
      "Unknown\n"
     ]
    }
   ],
   "source": [
    "print(stories[0].title)\n",
    "print(stories[0].author)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
